{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0DFSzioEQL2"
      },
      "source": [
        "Code for HW4_312551814"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCCbDD_jtqDt",
        "outputId": "87eb6477-90ea-4304-a206-1f20b14ee26f"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Attention, Permute\n",
        "from tensorflow.keras.models import Model, save_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Define constants\n",
        "BASE_PATH = r\"/content\"\n",
        "OUTPUT_PATH = r\"/kaggle/working/\"\n",
        "TRAIN_PATH = os.path.join(BASE_PATH, 'train')\n",
        "TEST_PATH = os.path.join(BASE_PATH, 'test')\n",
        "CLASS_0_PATH = os.path.join(TRAIN_PATH, 'class_0')\n",
        "CLASS_1_PATH = os.path.join(TRAIN_PATH, 'class_1')\n",
        "IMAGE_SIZE = (128, 128)\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "def load_images_from_pkl(file_path):\n",
        "    \"\"\"Load images from a pickle file.\"\"\"\n",
        "    with open(file_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return np.array(data)\n",
        "\n",
        "def load_data(class_paths):\n",
        "    \"\"\"Load training data and labels.\"\"\"\n",
        "    data, labels, files = [], [], []\n",
        "    for label, class_path in enumerate(class_paths):\n",
        "        for file_name in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, file_name)\n",
        "            images = load_images_from_pkl(file_path)\n",
        "            data.append(images)\n",
        "            labels.append(label)\n",
        "            files.append(file_name)\n",
        "    return np.array(data), np.array(labels), files\n",
        "\n",
        "def extract_features(model, images):\n",
        "    \"\"\"Extract features using a pre-trained model.\"\"\"\n",
        "    processed_images = preprocess_input(images)\n",
        "    features = model.predict(processed_images)\n",
        "    flattened_features = features.reshape((features.shape[0], -1))\n",
        "    return flattened_features\n",
        "\n",
        "def create_efficientnet_model(input_shape):\n",
        "    \"\"\"Create an EfficientNet model without the top layer.\"\"\"\n",
        "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    return Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "def create_submission_file(test_files, predictions, output_folder):\n",
        "    \"\"\"Create the submission file.\"\"\"\n",
        "    output_file = os.path.join(output_folder, 'submission.csv')\n",
        "    with open(output_file, 'w') as f:\n",
        "        f.write('image_id,y_pred\\n')\n",
        "        for file, pred in zip(test_files, predictions):\n",
        "            file_id = os.path.splitext(file)[0]\n",
        "            f.write(f'{file_id},{pred}\\n')\n",
        "    print(\"Submission file created successfully.\")\n",
        "\n",
        "def mil_attention_pooling(input_shape):\n",
        "    \"\"\"Create an MIL model with attention mechanism.\"\"\"\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    attention_probs = Dense(input_shape[0], activation='softmax', name='attention_vec')(input_layer)\n",
        "    attention_mul = tf.keras.layers.multiply([input_layer, attention_probs])\n",
        "    return Model(inputs=input_layer, outputs=attention_mul)\n",
        "\n",
        "def main():\n",
        "    # Load training data\n",
        "    train_data, train_labels, _ = load_data([CLASS_0_PATH, CLASS_1_PATH])\n",
        "\n",
        "    # Load test data\n",
        "    test_data, _, test_files = load_data([TEST_PATH])\n",
        "\n",
        "    # Create EfficientNet model\n",
        "    efficientnet_model = create_efficientnet_model((*IMAGE_SIZE, 3))\n",
        "\n",
        "    # Extract features for training data\n",
        "    train_features = [extract_features(efficientnet_model, bag) for bag in train_data]\n",
        "    train_features = np.array(train_features)\n",
        "    flattened_train_features = train_features.reshape((train_features.shape[0], -1))\n",
        "\n",
        "    # Split data into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(flattened_train_features, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Initialize classifiers\n",
        "    classifiers = {\n",
        "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        \"SVC\": SVC(kernel='linear', random_state=42),\n",
        "        \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
        "    }\n",
        "\n",
        "    # Train classifiers and evaluate\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        clf.fit(X_train, y_train)\n",
        "        accuracy = clf.score(X_val, y_val)\n",
        "        print(f'Validation accuracy {clf_name}: {accuracy}')\n",
        "\n",
        "    # Choose the best model based on validation accuracy\n",
        "    best_clf_name = max(classifiers, key=lambda clf_name: classifiers[clf_name].score(X_val, y_val))\n",
        "    best_clf = classifiers[best_clf_name]\n",
        "    print(f'Best classifier: {best_clf_name}')\n",
        "\n",
        "    # Save the best classifier model\n",
        "    with open(os.path.join(OUTPUT_PATH, 'best_classifier.pkl'), 'wb') as f:\n",
        "        pickle.dump(best_clf, f)\n",
        "    print(f'{best_clf_name} model saved successfully.')\n",
        "\n",
        "    # Extract features for test data\n",
        "    test_features = [extract_features(efficientnet_model, bag) for bag in test_data]\n",
        "    test_features = np.array(test_features)\n",
        "    flattened_test_features = test_features.reshape((test_features.shape[0], -1))\n",
        "\n",
        "    # Predict using the best classifier\n",
        "    test_predictions = best_clf.predict(flattened_test_features)\n",
        "\n",
        "    # Create submission file\n",
        "    create_submission_file(test_files, test_predictions, OUTPUT_PATH)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaDlSaSDEnjI"
      },
      "source": [
        "Question 1 code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hBAsWguuw-Uf",
        "outputId": "2780744c-eaee-4bbd-f893-96451fc2d723"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Attention, Permute\n",
        "from tensorflow.keras.models import Model, save_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "# Define constants\n",
        "BASE_PATH = r\"/content\"\n",
        "OUTPUT_PATH = r\"/content/\"\n",
        "TRAIN_PATH = os.path.join(BASE_PATH, 'train')\n",
        "TEST_PATH = os.path.join(BASE_PATH, 'test')\n",
        "CLASS_0_PATH = os.path.join(TRAIN_PATH, 'class_0')\n",
        "CLASS_1_PATH = os.path.join(TRAIN_PATH, 'class_1')\n",
        "IMAGE_SIZE = (128, 128)\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "def load_images_from_pkl(file_path):\n",
        "    \"\"\"Load images from a pickle file.\"\"\"\n",
        "    with open(file_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return np.array(data)\n",
        "\n",
        "def load_data(class_paths):\n",
        "    \"\"\"Load training data and labels.\"\"\"\n",
        "    data, labels, files = [], [], []\n",
        "    for label, class_path in enumerate(class_paths):\n",
        "        for file_name in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, file_name)\n",
        "            images = load_images_from_pkl(file_path)\n",
        "            data.append(images)\n",
        "            labels.append(label)\n",
        "            files.append(file_name)\n",
        "    return np.array(data), np.array(labels), files\n",
        "\n",
        "def extract_features(model, images):\n",
        "    \"\"\"Extract features using a pre-trained model.\"\"\"\n",
        "    processed_images = preprocess_input(images)\n",
        "    features = model.predict(processed_images)\n",
        "    flattened_features = features.reshape((features.shape[0], -1))\n",
        "    return flattened_features\n",
        "\n",
        "def create_efficientnet_model(input_shape):\n",
        "    \"\"\"Create an EfficientNet model without the top layer.\"\"\"\n",
        "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    return Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "def create_submission_file(test_files, predictions, output_folder):\n",
        "    \"\"\"Create the submission file.\"\"\"\n",
        "    output_file = os.path.join(output_folder, 'submission.csv')\n",
        "    with open(output_file, 'w') as f:\n",
        "        f.write('image_id,y_pred\\n')\n",
        "        for file, pred in zip(test_files, predictions):\n",
        "            file_id = os.path.splitext(file)[0]\n",
        "            f.write(f'{file_id},{pred}\\n')\n",
        "    print(\"Submission file created successfully.\")\n",
        "\n",
        "def mil_attention_pooling(input_shape):\n",
        "    \"\"\"Create an MIL model with attention mechanism.\"\"\"\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    attention_probs = Dense(input_shape[0], activation='softmax', name='attention_vec')(input_layer)\n",
        "    attention_mul = tf.keras.layers.multiply([input_layer, attention_probs])\n",
        "    return Model(inputs=input_layer, outputs=attention_mul)\n",
        "\n",
        "def plot_learning_curve(estimator, X, y, title=\"Learning Curve\", cv=5, n_jobs=None):\n",
        "    \"\"\"Plot learning curve for a given estimator.\"\"\"\n",
        "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=np.linspace(.1, 1.0, 5))\n",
        "\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.ylim(0.0, 1.1)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt\n",
        "\n",
        "def main():\n",
        "    # Load training data\n",
        "    train_data, train_labels, _ = load_data([CLASS_0_PATH, CLASS_1_PATH])\n",
        "\n",
        "    # Load test data\n",
        "    test_data, _, test_files = load_data([TEST_PATH])\n",
        "\n",
        "    # Create EfficientNet model\n",
        "    efficientnet_model = create_efficientnet_model((*IMAGE_SIZE, 3))\n",
        "\n",
        "    # Extract features for training data\n",
        "    train_features = [extract_features(efficientnet_model, bag) for bag in train_data]\n",
        "    train_features = np.array(train_features)\n",
        "    flattened_train_features = train_features.reshape((train_features.shape[0], -1))\n",
        "\n",
        "    # Split data into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(flattened_train_features, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Initialize classifiers\n",
        "    classifiers = {\n",
        "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "        \"SVC\": SVC(kernel='linear', random_state=42),\n",
        "        \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
        "    }\n",
        "\n",
        "    # Train classifiers, evaluate, and plot learning curves\n",
        "    best_accuracy = 0\n",
        "    best_clf_name = \"\"\n",
        "    best_clf = None\n",
        "\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        clf.fit(X_train, y_train)\n",
        "        val_predictions = clf.predict(X_val)\n",
        "\n",
        "        # Calculate evaluation metrics\n",
        "        accuracy = accuracy_score(y_val, val_predictions)\n",
        "        precision = precision_score(y_val, val_predictions)\n",
        "        recall = recall_score(y_val, val_predictions)\n",
        "        f1 = f1_score(y_val, val_predictions)\n",
        "\n",
        "        print(f'Validation metrics for {clf_name}:')\n",
        "        print(f'Accuracy: {accuracy:.4f}')\n",
        "        print(f'Precision: {precision:.4f}')\n",
        "        print(f'Recall: {recall:.4f}')\n",
        "        print(f'F1-score: {f1:.4f}\\n')\n",
        "\n",
        "        # Plot learning curve\n",
        "        plot_learning_curve(clf, X_train, y_train, title=f\"Learning Curve for {clf_name}\")\n",
        "\n",
        "        # Save the best classifier based on accuracy\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_clf_name = clf_name\n",
        "            best_clf = clf\n",
        "\n",
        "    print(f'Best classifier: {best_clf_name}')\n",
        "\n",
        "    # Save the best classifier model\n",
        "    with open(os.path.join(OUTPUT_PATH, 'best_classifier.pkl'), 'wb') as f:\n",
        "        pickle.dump(best_clf, f)\n",
        "    print(f'{best_clf_name} model saved successfully.')\n",
        "\n",
        "    # Perform ablation study by modifying one feature and evaluating the impact\n",
        "    def ablation_study(feature_modification_func):\n",
        "        modified_train_features = feature_modification_func(flattened_train_features)\n",
        "        X_train_mod, X_val_mod, y_train_mod, y_val_mod = train_test_split(modified_train_features, train_labels, test_size=0.2, random_state=42)\n",
        "        best_clf.fit(X_train_mod, y_train_mod)\n",
        "        val_predictions_mod = best_clf.predict(X_val_mod)\n",
        "        accuracy_mod = accuracy_score(y_val_mod, val_predictions_mod)\n",
        "        print(f'Ablation study accuracy with feature modification: {accuracy_mod:.4f}')\n",
        "\n",
        "    # Example ablation: removing the last feature\n",
        "    def remove_last_feature(features):\n",
        "        return features[:, :-1]\n",
        "\n",
        "    # Perform ablation on the validation set\n",
        "    ablation_study(remove_last_feature)\n",
        "\n",
        "    # Extract features for test data\n",
        "    test_features = [extract_features(efficientnet_model, bag) for bag in test_data]\n",
        "    test_features = np.array(test_features)\n",
        "    flattened_test_features = test_features.reshape((test_features.shape[0], -1))\n",
        "\n",
        "    # Apply the same ablation to the test set\n",
        "    flattened_test_features = remove_last_feature(flattened_test_features)\n",
        "\n",
        "    # Predict using the best classifier\n",
        "    test_predictions = best_clf.predict(flattened_test_features)\n",
        "\n",
        "    # Create submission file\n",
        "    create_submission_file(test_files, test_predictions, OUTPUT_PATH)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
